meta:
  exp_name: BallBalancing  # the name of this experiment
  run_name: None # (computed in runtime)
  seed: 1   # seed of the experiment 
  torch_deterministic: True   # if toggled, `torch.backends.cudnn.deterministic=False` 
  cuda: False   # if toggled, cuda will be enabled by default 
  track: True   # if toggled, this experiment will be tracked with Comet.ml
  # capture_video: False   # whether to capture videos of the agent performances (check out `videos` folder)
  save_model: True   # whether to save model into the `runs/{run_name}` folder
  env_id: "Widow250BallBalancing-v0"   # the id of the environment
  timelimit: 200
  sac_instead: False
  randomize_after_steps: 6_000_000
  add_to_runname: "wobble"
  from_pretrained: False

# Algorithm specific arguments
ppo:
  num_envs: 32
  eval_frequency: 200_000
  total_timesteps: 10_000_000   # total timesteps of the experiments
  learning_rate: 3e-4   # the learning rate of the optimizer 
  num_steps: 2048   # the number of steps to run in each environment per policy rollout
  anneal_lr: True   # Toggle learning rate annealing for policy and value networks 
  gamma: 0.99   # the discount factor gamma 
  gae_lambda: 0.95   # the lambda for the general advantage estimation 
  num_minibatches: 32   # the number of mini-batches 
  update_epochs: 10   # the K epochs to update the policy 
  norm_adv: True   # Toggles advantages normalization 
  clip_coef: 0.2   # the surrogate clipping coefficient 
  clip_vloss: True   # Toggles whether or not to use a clipped loss for the value function, as per the paper. 
  ent_coef: 0.0   # coefficient of the entropy 
  vf_coef: 0.5   # coefficient of the value function 
  max_grad_norm: 0.5   # the maximum norm for the gradient clipping 
  target_kl: None   # the target KL divergence threshold 

sac:
  num_envs: 8
  total_timesteps: 1_000_000 #total timesteps of the experiments
  buffer_size: 1e6 #the replay memory buffer size
  gamma: 0.99 #the discount factor gamma
  tau: 0.005 #target smoothing coefficient (default: 0.005)
  batch_size: 256 #the batch size of sample from the replay memory
  learning_starts: 5e3 #timestep to start learning
  policy_lr: 3e-4 #the learning rate of the policy network optimizer
  q_lr: 1e-3 #the learning rate of the Q network network optimizer
  policy_frequency: 2 #the frequency of training policy (delayed) Denis Yarats' implementation delays this by 2.
  target_network_frequency: 1  #  the frequency of updates for the target nerworks
  noise_clip: 0.5 #noise clip parameter of the Target Policy Smoothing Regularization
  alpha: 0.2 #Entropy regularization coefficient.
  autotune: True #automatic tuning of the entropy coefficient

# to be filled in runtime
batch_size: 0   # the batch size (computed in runtime) 
minibatch_size: 0   # the mini-batch size (computed in runtime) 
num_iterations: 0   # the number of iterations (computed in runtime) 

sim2real:
  history_len: 2
  add_last_action: True
  gaussian_obs_scale: 0.01
  gaussian_act_scale: 0.01
  action_repeat_max: 2
  max_action_repeat_on_reset: 8

environment:
  randomize_every_step: true
  randomize_ball_drop: true
  ball_size: 
    min: 0.4
    max: 0.9
  ball_mass: 
    reset: [0.00005, 0.005]
  ball_rolling_friction: 
    reset: [0.1, 1]
    step: [0.09, 0.11]
  ball_lateral_friction: 
    reset: [0.2, 0.8]
    step: [0.3, 0.5]
  ball_spinning_friction:
    reset: [0.01, 0.5]
    step: [0.008, 0.012]
  plate_lateral_friction: 
    reset: [0.1, 0.6]
    step: [0.17, 0.22]
  center_radius: 0.02
  distance_center_weight: 1.0
  duration_weight: 0.4
  height_weight: 0
  tilt_weight: 20